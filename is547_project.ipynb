{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IS547 Project Jupyter Notebook\n",
    "\n",
    "<details>\n",
    "<summary>Project Overview</summary>\n",
    "\n",
    "This project involves managing approximately 2200 digital documents originating from an internal WordPress site migration at my workplace. As previously outlined in my Dataset Profile, the data consists of PDFs, Word documents, Excel spreadsheets, and occasionally PowerPoint presentations already archived in our Box storage. These were curated over a decade or more by our seventy-plus library committees, albeit the majority of the data comes from 10-15 committees. The documents include meeting minutes, agendas, and related institutional records. With FAIR in mind, the curation goals I have are to enhance internal accessibility, maintain institutional memory and data provenance, and support governance through improved data organization and documentation. These documents were publicly available via our open staff site.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Deliverables</summary>\n",
    "\n",
    "- Consistent naming conventions applied across all documents\n",
    "- Documentation of data governance and ethical compliance per our institutional policies; if none exist, resources from university-wide policies will be utilized\n",
    "- Metadata enhancement to improve retrieval, searchability, and discoverability\n",
    "- Documented provenance and fixity check to support institutional memory\n",
    "\n",
    "</details>\n",
    "\n"
   ],
   "id": "e53a850fa5c66914"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that all code is importing functions from the data_pipeline package where several python files contain functions, sorted by file according to their purpose.",
   "id": "32ec556ef8aaa53f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First I get a total file count to check against later.",
   "id": "101a583fc41d5ca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "\n",
    "committees_directory = 'data/Committees'\n",
    "total_files = count_files(committees_directory)\n",
    "print(f\"Total number of files in '{committees_directory}': {total_files}\")"
   ],
   "id": "60d7447ebd07854a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next I review the file types in the data set.",
   "id": "b2d5fb52926672b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import find_file_types\n",
    "file_types = find_file_types('data/Committees')\n",
    "print(file_types)"
   ],
   "id": "58527f819ea45994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A list of committees and their count is helpful to make sure everything looks as it should (81 committees)",
   "id": "d35761a56117b89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import list_committees_and_count\n",
    "list_committees_and_count('data/Committees')\n"
   ],
   "id": "2e0372caa0b11a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then a list of files just to see what I'm working with.",
   "id": "32209b8b9f746a76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import list_files\n",
    "\n",
    "list_files('data/Committees')\n"
   ],
   "id": "8f2a8bd43bf968dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A function to ensure files are delivered to the right place so no mess is created.",
   "id": "a012590d4da06919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import ensure_output_directory, clean_ds_store_files\n",
    "\n",
    "ensure_output_directory()\n",
    "\n"
   ],
   "id": "b63e97079f655c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I copy the original files to the processed directory.  This ensures the original data set is untouched.",
   "id": "c46e69bc94d565c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import copy_files\n",
    "\n",
    "copy_files()"
   ],
   "id": "a54dd6771beddaf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count files again to verify the copy was successful.",
   "id": "ec7b8fa70261c87b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "count_files('data/Processed_Committees')"
   ],
   "id": "793d53887cbfdb75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Review file types again to see if anything changed.",
   "id": "da2fd31b9fcc85a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import find_file_types\n",
    "file_types = find_file_types('data/Processed_Committees')\n",
    "print(file_types)"
   ],
   "id": "356f7ba76ceba12f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function creates a CSV with committee, type, original filename, extracted date, and proposed filename.  The CSV is \"names.csv\" and placed in the data directory.  From examining the CSV data I can see:\n",
    "1. **Related Documents** There are a significant number of files in \"Related Documents\" folders. These maintain their original, often unique filenames and are skipped during renaming.\n",
    "2. **\"Unknown\" Date Files**: Many files have \"unknown\" in their proposed filenames (especially from committees like \"Diversity Residency Advisory Committee\" and \"DEIA Task Force\"). These would standardize to the same pattern, reducing unique names.\n",
    "3. **Duplicate Resolution**: Files like and would be normalized to the same standardized name, with collision handling adding suffixes as needed. `capt_agenda_minutes_2013_04_30.docx``capt_agenda_minutes_2013_04_30 (1).docx`\n",
    "\n",
    "The reduction (2193 - 1610 = 583 fewer unique values) indicates that about 26.6% of the original filenames were standardized or excluded from renaming (like Related Documents), which is expected in a file organization project focused on consistent naming.  This indicates the standardization process is successfully reducing naming inconsistencies while preserving the original files in Related Documents folders that likely need their distinct names for context.\n"
   ],
   "id": "808c596be2a81997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.file_naming import generate_names_csv\n",
    "\n",
    "generate_names_csv()"
   ],
   "id": "8abdc0a26c6ab824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Again I list files to see if anything has changed.",
   "id": "7f5991b329b93b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import list_files\n",
    "\n",
    "list_files(\"./data/Processed_Committees\")"
   ],
   "id": "604e5015556b9a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is where I update the filenames based on the CSV created in the previous step.  This comes after the hours I spent manually cleaning the data and adding dates the hard way to the date column in the \"names.csv\" and renamed it \"manually_updated_committee_names.csv\"  It adds a column for the final concatenated names and saves the updated CSV \"final_updated_committee_names.csv\" in the data directory.  NOTE HOW CLOSE THE UNIQUE VLAUES ARE BEGINNING TO END.",
   "id": "cb551a328811c6aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import build_final_filenames\n",
    "build_final_filenames()"
   ],
   "id": "6d54c2a1148f8f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I verify the folder structure and files are as expected before the final renaming.",
   "id": "f590c9ca1064f1a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import verify_folder_file_structure\n",
    "verify_folder_file_structure()"
   ],
   "id": "4d83aa76b6918124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The big event - renaming the files.  It renames less than the full amount as some of the new file names match the old, and Related Docs never got renamed due to unique naming with no dates in many cases.",
   "id": "2d6be8a8127b0c34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import rename_processed_files\n",
    "rename_processed_files()"
   ],
   "id": "7a5c6ea56f1e97e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When checked manually the file names with dates appended appear to work exactly as I want.",
   "id": "b21eb3927fdce8c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import list_files\n",
    "\n",
    "list_files(\"./data/Processed_Committees\")"
   ],
   "id": "161cc68d9e8f32ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Validate the same number of files exist as when we started:\n",
   "id": "768ec8ac27d36142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "count_files('data/Processed_Committees')"
   ],
   "id": "44f7952ff2d139c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enhance file metadata with json-ld files",
   "id": "4b4fc1822d7bba83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the module\n",
    "from data_pipeline import enhance_metadata\n",
    "\n",
    "# Call the single combined function instead of both separately\n",
    "enhance_metadata.enhance_all_metadata(\n",
    "    csv_path=\"data/final_updated_committee_names.csv\",\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    skip_existing=False\n",
    ")\n"
   ],
   "id": "e663493788fa644b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enhance Project Metadata by creating a json-ld file in the root directory with basic description of the project",
   "id": "92a838cc6b683110"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.project_metadata import write_project_metadata\n",
    "write_project_metadata()\n"
   ],
   "id": "103e471ff2388d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NLP term extraction to create a preview of entities in the data set.  This is a first step in identifying key terms and concepts for further analysis.",
   "id": "380be964fb91e1e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.nlp_term_extraction_preview import run_entity_preview\n",
    "run_entity_preview()"
   ],
   "id": "a5729b9d9fd353d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next I test the enhance_json_with_nlp function quickly before running the full process.",
   "id": "2c3f5656e9d30265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.add_nlp_terms_to_metadata import enhance_json_with_nlp\n",
    "\n",
    "# Update a small sample of JSON-LD files first as a test\n",
    "# Using a limit of 10 files to see quick results\n",
    "test_results = enhance_json_with_nlp(base_dir=\"data/Processed_Committees\", limit=10)\n",
    "\n",
    "# Review the test results\n",
    "print(\"\\nTest completed. Check the output above to see if it looks correct.\")"
   ],
   "id": "c3e20577a637911f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a big event, takes several minutes, where we do batch processing of the files for term extraction and add the terms to the json-ld",
   "id": "50294fa7c54feb4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.add_nlp_terms_to_metadata import enhance_json_with_nlp\n",
    "\n",
    "# First batch - process 500 files\n",
    "enhance_json_with_nlp(\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    limit=2500,\n",
    "    skip_existing=False,\n",
    ")\n"
   ],
   "id": "d2820f024d425457",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets make sure no Mac .DS_Store files are contaminating the set.",
   "id": "c1989c1bf01f1c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "from data_pipeline import data_cleaning\n",
    "importlib.reload(data_cleaning)\n",
    "\n",
    "# Then try calling it\n",
    "data_cleaning.clean_ds_store_files()"
   ],
   "id": "2cd97ac258c3d46b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build knowledge graph with redacted set of data.",
   "id": "fea2deb47ab987bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the functions\n",
    "from data_pipeline.build_redacted_knowlege_graph import create_person_document_explorer\n",
    "\n",
    "# Create an interactive knowledge graph\n",
    "graph, network = create_person_document_explorer(\n",
    "    base_dir=\"data/Processed_Committees/Executive Committee\",\n",
    "    committee=None,  # All committees\n",
    "    limit=50,       # Process up to 100 files\n",
    "    min_person_mentions=2,\n",
    "    output_file=\"knowledge_graph_explorer.html\"\n",
    ")"
   ],
   "id": "5645e587bb19bdb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build the final knowledge graph with added entities.",
   "id": "1fa80f8831ecd1ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.metadata_check import check_person_entities\n",
    "check_person_entities()"
   ],
   "id": "baaae39f4994ccfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.add_nlp_terms_to_metadata import reprocess_all_entities\n",
    "\n",
    "result = reprocess_all_entities(\n",
    "    report_path=\"data/nlp_quality_report.json\"\n",
    "  )\n",
    "\n",
    "\n",
    "import json\n",
    "with open(\"data/nlp_quality_report.json\") as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(f\"Low quality documents: {len(report['problematic_documents'])}\")\n",
    "print(f\"PERSON rejection rate: {report['entity_stats']['PERSON']['rejection_rate']:.1%}\")\n"
   ],
   "id": "14e11dd47a11f746",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Neo4j Export\n",
    "from data_pipeline.neo4j_export import export_to_neo4j\n",
    "\n",
    "result = export_to_neo4j(\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    output_format=\"both\",  # generates both .cypher and CSV files\n",
    "    min_person_mentions=2,\n",
    "    min_coappear_count=2\n",
    ")\n",
    "\n",
    "print(f\"Cypher file: {result.get('cypher')}\")\n",
    "print(f\"CSV directory: {result.get('csv_dir')}\")\n",
    "\n"
   ],
   "id": "9d8a759b44526e01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Neo4j Direct Import\n",
    "from data_pipeline.neo4j_import import import_to_neo4j\n",
    "\n",
    "stats = import_to_neo4j(\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    min_person_mentions=2,\n",
    "    clear_first=True  # Clears existing data before import\n",
    "  )\n"
   ],
   "id": "80a13d53d3acd681",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3nikmqvm6kw",
   "source": "## Graph Dataset Preparation for GraphRAG\n\nThe following cells create a filtered, cleaned dataset specifically for knowledge graph and GraphRAG applications:\n\n1. **Filter Minutes Only** - Extract only Minutes documents (excluding Agendas and Related Documents) into a flattened structure\n2. **Clean Entities** - Apply stricter NLP entity validation to remove:\n   - Single-word names (first names only)\n   - Acronyms and abbreviations  \n   - Misclassified entities (persons as ORG/GPE)\n   - Contraction artifacts and garbage text\n\nThis creates `data/committees_processed_for_graph/` with cleaner entity data for semantic search and Q&A.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vstfqrmotbs",
   "source": "# Step 1: Filter to Minutes-only dataset\n# Creates data/committees_processed_for_graph/ with flattened structure\n\nfrom data_pipeline.filter_for_graph import filter_for_graph\n\nfilter_result = filter_for_graph(\n    source_dir=\"data/Processed_Committees\",\n    dest_dir=\"data/committees_processed_for_graph\"\n)\n\nprint(f\"\\nReady for entity cleanup: {filter_result['documents_copied']} documents\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ihwgcivyt",
   "source": "# Step 2: Clean up entities with stricter validation\n# Reprocesses NLP entities with filters for:\n# - Single-word names, acronyms, generic terms\n# - Misclassified persons in ORG/GPE\n# - Contraction artifacts (n't)\n\nfrom data_pipeline.cleanup_graph_entities import cleanup_graph_entities\n\ncleanup_result = cleanup_graph_entities(\n    base_dir=\"data/committees_processed_for_graph\",\n    report_path=\"data/graph_nlp_quality_report.json\"\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "rjlqcfnncko",
   "source": "# Step 3: Review cleaned entities\n# Verify entity quality after cleanup\n\nfrom data_pipeline.cleanup_graph_entities import show_top_entities\n\nshow_top_entities(\n    base_dir=\"data/committees_processed_for_graph\",\n    top_n=15\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "u3vbpnhnypk",
   "source": "### Graph Dataset Summary\n\nThe filtered and cleaned dataset is now ready at `data/committees_processed_for_graph/`:\n\n- **Documents:** ~1,143 Minutes files from 26 committees\n- **Structure:** Flattened `[Committee Name]/[files]` (no subfolders)\n- **Entity Quality:**\n  - PERSON: Full names only (Tom Teper, John Wilkin, etc.)\n  - ORG: Real organizations (User Education Committee, Administrative Council, etc.)\n  - GPE: Real locations (Illinois, Chicago, etc.)\n\n**Next Steps:** This dataset is ready for GraphRAG integration with Ollama embeddings. See `docs/features/feature-graphrag-ollama-integration.md` for the implementation plan.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bo2yk3sisj7",
   "source": "# Step 4: Re-import cleaned data to Neo4j\n# This replaces the original import with the cleaned Minutes-only dataset\n# Note: This clears existing data including embeddings - you'll need to regenerate them\n\nfrom data_pipeline.neo4j_import import import_to_neo4j\n\nstats = import_to_neo4j(\n    base_dir=\"data/committees_processed_for_graph\",  # Use cleaned dataset\n    min_person_mentions=2,\n    clear_first=True  # Clear old dirty data first\n)\n\nprint(\"\\nNeo4j now contains cleaned entities from Minutes-only dataset.\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0i3g8tjiaat8",
   "source": "## GraphRAG Integration with Ollama\n\nThis section adds semantic search and question-answering capabilities using:\n- **neo4j-graphrag-python**: Official Neo4j GraphRAG library\n- **Ollama**: Local LLM and embeddings (privacy-preserving)\n- **Vector Index**: Semantic similarity search on document embeddings\n- **Hybrid Retrieval**: Combines vector search + keyword matching + graph traversal\n\n**Prerequisites:**\n1. Ollama installed and running (`ollama serve`)\n2. Models downloaded: `ollama pull nomic-embed-text` and `ollama pull llama3.1:8b`\n3. Neo4j database populated (run previous cells first)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9113eov8bbd",
   "source": "# Step 1: Verify GraphRAG prerequisites\n# Checks Ollama connection, Neo4j connection, and available models\n\nfrom data_pipeline.graphrag_verification import run_full_verification\n\nverification = run_full_verification(verbose=True)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "qt3wbklfuv",
   "source": "# Step 2: Generate embeddings for documents\n# This processes all documents and stores embeddings in Neo4j\n# Takes ~60-90 minutes for full dataset, can be interrupted and resumed\n\nfrom data_pipeline.generate_embeddings import (\n    setup_embedder,\n    extract_and_embed_documents,\n    check_embedding_status\n)\n\n# Setup embedder\nembedder_config = setup_embedder()\n\n# Generate embeddings (skip existing to allow resume)\nembedding_stats = extract_and_embed_documents(\n    base_dir=\"data/committees_processed_for_graph\",\n    skip_existing=True,  # Set to False to regenerate all\n    batch_size=50\n)\n\n# Check status\ncheck_embedding_status()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "uabscug2lk",
   "source": "# Step 3: Create vector and fulltext indexes\n# Required for semantic search operations\n\nfrom data_pipeline.setup_vector_index import setup_all_indexes\n\nindex_results = setup_all_indexes(\n    vector_index_name=\"document_embeddings\",\n    fulltext_index_name=\"document_fulltext\",\n    wait_for_online=True\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80h52x9vy3h",
   "source": "# Step 4: Test semantic search\n# Verify vector retrieval is working\n\nfrom data_pipeline.graphrag_retriever import GraphRAGRetriever\n\nwith GraphRAGRetriever() as retriever:\n    # Test query\n    query = \"What budget discussions took place?\"\n    results = retriever.search(query, top_k=5, search_type=\"graph_enhanced\")\n    \n    print(f\"Query: {query}\\n\")\n    print(f\"Results ({len(results)} documents):\\n\")\n    \n    for i, doc in enumerate(results, 1):\n        print(f\"{i}. {doc['name']}\")\n        print(f\"   Date: {doc.get('date', 'N/A')}\")\n        print(f\"   Committee: {doc.get('committeeName') or doc.get('committee', 'N/A')}\")\n        print(f\"   Score: {doc.get('score', 'N/A'):.3f}\" if doc.get('score') else \"\")\n        if doc.get('mentionedPeople'):\n            print(f\"   People: {', '.join(doc['mentionedPeople'][:3])}\")\n        print()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "htwjeo68uj5",
   "source": "# Step 5: Ask questions with GraphRAG Q&A\n# Uses retrieval + LLM to answer questions about documents\n\nfrom data_pipeline.graphrag_qa import GraphRAGQA, example_questions\n\n# Show example questions\nprint(\"Example questions you can ask:\")\nfor i, q in enumerate(example_questions(), 1):\n    print(f\"  {i}. {q}\")\nprint()\n\n# Initialize Q&A system\nqa = GraphRAGQA()\n\n# Ask a question\nquestion = \"What were the main topics discussed in Executive Committee meetings?\"\nprint(f\"Question: {question}\\n\")\nprint(\"Generating answer...\\n\")\n\nresult = qa.ask(question, top_k=5)\n\nprint(f\"Answer:\\n{result['answer']}\\n\")\nprint(f\"Sources ({len(result['sources'])} documents):\")\nfor i, src in enumerate(result['sources'], 1):\n    print(f\"  {i}. {src['name']}\")\n    if src.get('committee'):\n        print(f\"     Committee: {src['committee']}\")\n\nqa.close()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "yasxen9a0db",
   "source": "### GraphRAG Capabilities\n\nThe system now supports:\n\n- **Semantic Search**: Find documents by meaning, not just keywords\n- **Graph-Enhanced Retrieval**: Includes committee context and mentioned people\n- **Natural Language Q&A**: Ask questions and get answers with source citations\n- **Person Search**: Find all documents mentioning a specific person\n- **Committee Search**: Find all documents from a specific committee\n\n**Example Usage:**\n```python\nfrom data_pipeline.graphrag_qa import GraphRAGQA\n\nwith GraphRAGQA() as qa:\n    result = qa.ask(\"What technology initiatives were discussed?\")\n    print(result['answer'])\n```\n\n**Interactive Mode:**\n```python\nfrom data_pipeline.graphrag_qa import interactive_qa\ninteractive_qa()  # Starts interactive Q&A session\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0kcabhbd3a8s",
   "source": [
    "# Quick Q&A - Ask a single question\n",
    "from data_pipeline.graphrag_qa import GraphRAGQA\n",
    "\n",
    "# Change the question to whatever you want to ask\n",
    "question = (\"what can you find on new building project\")\n",
    "\n",
    "with GraphRAGQA() as qa:\n",
    "    result = qa.ask(question, top_k=25\n",
    "                    )\n",
    "    \n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "    print(f\"Sources ({len(result['sources'])}):\")\n",
    "    for s in result['sources']:\n",
    "        print(f\"  - {s['name']}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:55:14.706686Z",
     "start_time": "2026-01-27T12:54:31.588713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM configured: llama3.1:8b, temperature=0.1\n",
      "Question: what can you find on new building project\n",
      "\n",
      "Answer:\n",
      "After analyzing the documents, I found several mentions of a \"Library Building Project\" or \"Main Library Building\". Here are some specific details:\n",
      "\n",
      "* Document 2 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-03-26.docx) mentions the \"Library Building Project: Programming\" organization.\n",
      "* Document 3 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-02-04.docx) mentions the \"Library Building Project: Programming\" organization and also lists \"Exhibit-Public Event\" as an organization related to this project.\n",
      "* Document 4 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-07-08.docx) mentions the \"Library Building Project: Programming\" organization.\n",
      "* Document 5 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-02-25.docx) mentions the \"Library Building Project: Programming\" organization and also lists \"Special Collections, Maps\" as related organizations.\n",
      "* Document 11 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-05-28.docx) mentions the \"Main Library\" organization.\n",
      "* Document 21 (The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-04-30.docx) mentions the \"Library Building Project: Programming\" and also lists \"Ithaka\" as an organization related to this project.\n",
      "\n",
      "It appears that there is a new building project, likely for the main library, which involves various organizations such as the Library Building Project, Main Library, Special Collections, Maps, and Ithaka.\n",
      "\n",
      "Sources (25):\n",
      "  - Library Staff Support Committee_Minutes_2018-09-27.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-03-26.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-02-04.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-07-08.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-02-25.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-06-06.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-01-22.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-01-07.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-03-07.docx\n",
      "  - The Library as Catalyst Project - Special Collections Research Center Working Group_Minutes_2019-05-30.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-05-28.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-05-15.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-05-02.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-02-20.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-02-05.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-04-29.docx\n",
      "  - Library Assessment Committee_Minutes_2018-10-11.docx\n",
      "  - Library Staff Support Committee_Minutes_2019-09-26.docx\n",
      "  - The Library as Catalyst Project - Special Collections Research Center Working Group_Minutes_2019-10-29.docx\n",
      "  - The Library as Catalyst Project - Managing the Library's Collections WG_Minutes_2019-06-26.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-04-30.docx\n",
      "  - Library Assessment Committee_Minutes_2019-01-09.docx\n",
      "  - Library Assessment Committee_Minutes_2019-11-07.docx\n",
      "  - The Library as Catalyst Project_ Programming the Main Library Building WG_Minutes_2019-10-29.docx\n",
      "  - The Library as Catalyst Project - Special Collections Research Center Working Group_Minutes_2019-06-13.docx\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "flajmoemwhu",
   "source": "from data_pipeline.graphrag_retriever import get_neo4j_driver\n\ndriver = get_neo4j_driver()\nwith driver.session() as session:\n    result = session.run(\"\"\"\n        MATCH (p:Person)\n        RETURN p.name AS name, p.mentionCount AS mentions\n        ORDER BY p.mentionCount DESC\n        LIMIT 10\n    \"\"\")\n    \n    for record in result:\n        print(f\"{record['name']}: {record['mentions']} mentions\")\n\ndriver.close()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T12:45:39.711184Z",
     "start_time": "2026-01-27T12:45:39.517090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom Teper: 379 mentions\n",
      "John Wilkin: 325 mentions\n",
      "Mary Laskowski: 317 mentions\n",
      "Bill Mischo: 257 mentions\n",
      "David Ward: 256 mentions\n",
      "Sue Searing: 190 mentions\n",
      "Lynne Rudasill: 179 mentions\n",
      "Mara Thacker: 175 mentions\n",
      "Chris Prom: 170 mentions\n",
      "Jennifer Teper: 166 mentions\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
