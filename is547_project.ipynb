{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IS547 Project Jupyter Notebook\n",
    "\n",
    "<details>\n",
    "<summary>Project Overview</summary>\n",
    "\n",
    "This project involves managing approximately 2200 digital documents originating from an internal WordPress site migration at my workplace. As previously outlined in my Dataset Profile, the data consists of PDFs, Word documents, Excel spreadsheets, and occasionally PowerPoint presentations already archived in our Box storage. These were curated over a decade or more by our seventy-plus library committees, albeit the majority of the data comes from 10-15 committees. The documents include meeting minutes, agendas, and related institutional records. With FAIR in mind, the curation goals I have are to enhance internal accessibility, maintain institutional memory and data provenance, and support governance through improved data organization and documentation. These documents were publicly available via our open staff site.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Deliverables</summary>\n",
    "\n",
    "- Consistent naming conventions applied across all documents\n",
    "- Documentation of data governance and ethical compliance per our institutional policies; if none exist, resources from university-wide policies will be utilized\n",
    "- Metadata enhancement to improve retrieval, searchability, and discoverability\n",
    "- Documented provenance and fixity check to support institutional memory\n",
    "\n",
    "</details>\n",
    "\n"
   ],
   "id": "e53a850fa5c66914"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that all code is importing functions from the data_pipeline package where several python files contain functions, sorted by file according to their purpose.",
   "id": "32ec556ef8aaa53f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First I get a total file count to check against later.",
   "id": "101a583fc41d5ca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "\n",
    "committees_directory = 'data/Committees'\n",
    "total_files = count_files(committees_directory)\n",
    "print(f\"Total number of files in '{committees_directory}': {total_files}\")"
   ],
   "id": "60d7447ebd07854a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next I review the file types in the data set.",
   "id": "b2d5fb52926672b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import find_file_types\n",
    "file_types = find_file_types('data/Committees')\n",
    "print(file_types)"
   ],
   "id": "58527f819ea45994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A list of committees and their count is helpful to make sure everything looks as it should (81 committees)",
   "id": "d35761a56117b89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import list_committees_and_count\n",
    "list_committees_and_count('data/Committees')\n"
   ],
   "id": "2e0372caa0b11a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then a list of files just to see what I'm working with.",
   "id": "32209b8b9f746a76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import list_files\n",
    "\n",
    "list_files('data/Committees')\n"
   ],
   "id": "8f2a8bd43bf968dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A function to ensure files are delivered to the right place so no mess is created.",
   "id": "a012590d4da06919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import ensure_output_directory, clean_ds_store_files\n",
    "\n",
    "ensure_output_directory()\n",
    "\n"
   ],
   "id": "b63e97079f655c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I copy the original files to the processed directory.  This ensures the original data set is untouched.",
   "id": "c46e69bc94d565c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import copy_files\n",
    "\n",
    "copy_files()"
   ],
   "id": "a54dd6771beddaf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count files again to verify the copy was successful.",
   "id": "ec7b8fa70261c87b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "count_files('data/Processed_Committees')"
   ],
   "id": "793d53887cbfdb75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Review file types again to see if anything changed.",
   "id": "da2fd31b9fcc85a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import find_file_types\n",
    "file_types = find_file_types('data/Processed_Committees')\n",
    "print(file_types)"
   ],
   "id": "356f7ba76ceba12f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function creates a CSV with committee, type, original filename, extracted date, and proposed filename.  The CSV is \"names.csv\" and placed in the data directory.  From examining the CSV data I can see:\n",
    "1. : There are a significant number of files in \"Related Documents\" folders. These maintain their original, often unique filenames and are skipped during renaming. **Related Documents**\n",
    "2. **\"Unknown\" Date Files**: Many files have \"unknown\" in their proposed filenames (especially from committees like \"Diversity Residency Advisory Committee\" and \"DEIA Task Force\"). These would standardize to the same pattern, reducing unique names.\n",
    "3. **Duplicate Resolution**: Files like and would be normalized to the same standardized name, with collision handling adding suffixes as needed. `capt_agenda_minutes_2013_04_30.docx``capt_agenda_minutes_2013_04_30 (1).docx`\n",
    "\n",
    "The specific reduction (2193 - 1610 = 583 fewer unique values) indicates that about 26.6% of your original filenames were standardized or excluded from renaming (like Related Documents), which is expected in a file organization project focused on consistent naming.\n",
    "This is a positive outcome that indicates your standardization process is successfully reducing naming inconsistencies while preserving the original files in Related Documents folders that likely need their distinct names for context.\n"
   ],
   "id": "808c596be2a81997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.file_naming import generate_names_csv\n",
    "\n",
    "generate_names_csv()"
   ],
   "id": "8abdc0a26c6ab824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Again I list files to see if anything has changed.",
   "id": "7f5991b329b93b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import list_files\n",
    "\n",
    "list_files(\"./data/Processed_Committees\")"
   ],
   "id": "604e5015556b9a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is where I update the filenames based on the CSV created in the previous step.  This comes after the hours I spent manually cleaning the data and adding dates the hard way to the date column in the \"names.csv\" and renamed it \"manually_updated_committee_names.csv\"  It adds a column for the final concatenated names and saves the updated CSV \"final_updated_committee_names.csv\" in the data directory.  NOTE HOW CLOSE THE UNIQUE VLAUES ARE BEGINNING TO END.",
   "id": "cb551a328811c6aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import build_final_filenames\n",
    "build_final_filenames()"
   ],
   "id": "6d54c2a1148f8f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I verify the folder structure and files are as expected before the final renaming.",
   "id": "f590c9ca1064f1a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import verify_folder_file_structure\n",
    "verify_folder_file_structure()"
   ],
   "id": "4d83aa76b6918124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The big event - renaming the files.  It renames less than the full amount as some of the new file names match the old, and Related Docs never got renamed due to unique naming with no dates in many cases.",
   "id": "2d6be8a8127b0c34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.final_file_naming import rename_processed_files\n",
    "rename_processed_files()"
   ],
   "id": "7a5c6ea56f1e97e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When checked manually the file names with dates appended appear to work exactly as I want.",
   "id": "b21eb3927fdce8c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_cleaning import list_files\n",
    "\n",
    "list_files(\"./data/Processed_Committees\")"
   ],
   "id": "161cc68d9e8f32ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Validate the same number of files exist as when we started:\n",
   "id": "768ec8ac27d36142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.data_explore import count_files\n",
    "count_files('data/Processed_Committees')"
   ],
   "id": "44f7952ff2d139c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enhance file metadata with json-ld files",
   "id": "4b4fc1822d7bba83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the module\n",
    "from data_pipeline import enhance_metadata\n",
    "\n",
    "# Call the single combined function instead of both separately\n",
    "enhance_metadata.enhance_all_metadata(\n",
    "    csv_path=\"data/final_updated_committee_names.csv\",\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    skip_existing=False\n",
    ")\n"
   ],
   "id": "e663493788fa644b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enhance Project Metadata by creating a json-ld file in the root directory with basic description of the project",
   "id": "92a838cc6b683110"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.project_metadata import write_project_metadata\n",
    "write_project_metadata()\n"
   ],
   "id": "103e471ff2388d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NLP term extraction to create a preview of entities in the data set.  This is a first step in identifying key terms and concepts for further analysis.",
   "id": "380be964fb91e1e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.nlp_term_extraction_preview import run_entity_preview\n",
    "run_entity_preview()"
   ],
   "id": "a5729b9d9fd353d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next I test the enhance_json_with_nlp function quickly before running the full process.",
   "id": "2c3f5656e9d30265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.add_nlp_terms_to_metadata import enhance_json_with_nlp\n",
    "\n",
    "# Update a small sample of JSON-LD files first as a test\n",
    "# Using a limit of 10 files to see quick results\n",
    "test_results = enhance_json_with_nlp(base_dir=\"data/Processed_Committees\", limit=10)\n",
    "\n",
    "# Review the test results\n",
    "print(\"\\nTest completed. Check the output above to see if it looks correct.\")"
   ],
   "id": "c3e20577a637911f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a big event, takes several minutes, where we do batch processing of the files for term extraction and add the terms to the json-ld",
   "id": "50294fa7c54feb4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pipeline.add_nlp_terms_to_metadata import enhance_json_with_nlp\n",
    "\n",
    "# First batch - process 500 files\n",
    "enhance_json_with_nlp(\n",
    "    base_dir=\"data/Processed_Committees\",\n",
    "    limit=2500,\n",
    "    skip_existing=False\n",
    ")\n"
   ],
   "id": "d2820f024d425457",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets make sure no Mac .DS_Store files are contaminating the set.",
   "id": "c1989c1bf01f1c8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T12:27:02.854932Z",
     "start_time": "2025-05-31T12:27:02.814122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "from data_pipeline import data_cleaning\n",
    "importlib.reload(data_cleaning)\n",
    "\n",
    "# Then try calling it\n",
    "data_cleaning.clean_ds_store_files()"
   ],
   "id": "2cd97ac258c3d46b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No .DS_Store files found - directory is clean!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T23:26:35.586021Z",
     "start_time": "2025-05-31T23:26:35.555270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the functions\n",
    "from data_pipeline.build_redacted_knowlege_graph import create_person_document_explorer\n",
    "\n",
    "# Create an interactive knowledge graph\n",
    "graph, network = create_person_document_explorer(\n",
    "    base_dir=\"data/Processed_Committees/Executive Committee\",\n",
    "    committee=None,  # All committees\n",
    "    limit=50,       # Process up to 100 files\n",
    "    min_person_mentions=2,\n",
    "    output_file=\"knowledge_graph_explorer.html\"\n",
    ")"
   ],
   "id": "5645e587bb19bdb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building person-centric graph...\n",
      "Found 67 people with 2+ mentions\n",
      "Graph built: 117 nodes, 249 edges\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "knowledge_graph_explorer.html\n",
      "Interactive person explorer saved to knowledge_graph_explorer.html\n",
      "Instructions:\n",
      "- Click on any person (blue node) to see only their document connections\n",
      "- Click the same person again or click empty space to show all nodes\n",
      "- Hover over nodes to see detailed information\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T12:33:07.073425Z",
     "start_time": "2025-05-31T12:33:07.018748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_pipeline.metadata_check import check_person_entities\n",
    "check_person_entities()"
   ],
   "id": "baaae39f4994ccfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 20 files\n",
      "Current PERSON entities found:\n",
      "  Bill: 9 mentions\n",
      "  Joanne: 9 mentions\n",
      "  Lynne: 9 mentions\n",
      "  Tom: 9 mentions\n",
      "  Tom Teper: 9 mentions\n",
      "  Bill Maher: 8 mentions\n",
      "  Victor Jones: 8 mentions\n",
      "  Joanne Kaczmarek: 7 mentions\n",
      "  Kelli Trei: 7 mentions\n",
      "  Hannah Williams\n",
      "Box: 7 mentions\n",
      "  John: 6 mentions\n",
      "  Chris Wiley: 6 mentions\n",
      "  Mara Thacker: 6 mentions\n",
      "  Victor: 5 mentions\n",
      "  Lynne Thomas\n",
      "Absent: 4 mentions\n",
      "Checked 10 files\n",
      "Current PERSON entities found:\n",
      "  Bill: 9 mentions\n",
      "  Joanne: 9 mentions\n",
      "  Tom: 9 mentions\n",
      "  Tom Teper: 9 mentions\n",
      "  Bill Maher: 8 mentions\n",
      "  Lynne: 8 mentions\n",
      "  Joanne Kaczmarek: 7 mentions\n",
      "  John: 5 mentions\n",
      "  Lynne Thomas\n",
      "Absent: 4 mentions\n",
      "  Box: 3 mentions\n",
      "  Tim: 3 mentions\n",
      "  Wendy: 3 mentions\n",
      "  Krista: 2 mentions\n",
      "  Maps: 2 mentions\n",
      "  Sousa: 2 mentions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Bill': 9,\n",
       "         'Joanne': 9,\n",
       "         'Tom': 9,\n",
       "         'Tom Teper': 9,\n",
       "         'Bill Maher': 8,\n",
       "         'Lynne': 8,\n",
       "         'Joanne Kaczmarek': 7,\n",
       "         'John': 5,\n",
       "         'Lynne Thomas\\nAbsent': 4,\n",
       "         'Box': 3,\n",
       "         'Tim': 3,\n",
       "         'Wendy': 3,\n",
       "         'Krista': 2,\n",
       "         'Maps': 2,\n",
       "         'Sousa': 2,\n",
       "         'Aeon': 2,\n",
       "         'John’s': 2,\n",
       "         'Krista Gray': 2,\n",
       "         'Lynne Thomas': 2,\n",
       "         'Dennis': 2,\n",
       "         'Tim Newman': 2,\n",
       "         'Wendy Wolter': 2,\n",
       "         'William Maher': 2,\n",
       "         'Tom - I': 2,\n",
       "         'Archon': 1,\n",
       "         'Bill – Oak Street': 1,\n",
       "         'Bill – Voyager': 1,\n",
       "         'Cara Bertram': 1,\n",
       "         'Chris': 1,\n",
       "         'Joanne Kaczmarek\\nNote': 1,\n",
       "         'Katie Nichols': 1,\n",
       "         'Krannert': 1,\n",
       "         'Linda': 1,\n",
       "         'Linda Stahnke': 1,\n",
       "         'Mary': 1,\n",
       "         'Sarah Harris': 1,\n",
       "         'Solberg': 1,\n",
       "         'Training': 1,\n",
       "         'Valerie': 1,\n",
       "         'Wendy Wolter\\n\\nBill': 1,\n",
       "         'Alma': 1,\n",
       "         'Info': 1,\n",
       "         'Joanne Kaczmarek \\nBill': 1,\n",
       "         'John Laskowski': 1,\n",
       "         'Joanne Kaczmarek\\nLynne': 1,\n",
       "         'Lynne -  e': 1,\n",
       "         'Elizabeth': 1,\n",
       "         'Kim': 1,\n",
       "         'Espresso': 1,\n",
       "         'Harding Band Building': 1,\n",
       "         'Hort': 1,\n",
       "         'Lynne Thomas\\nNote': 1,\n",
       "         'Tom - John': 1,\n",
       "         'cubic feet': 1,\n",
       "         'Krista Gray\\nNote': 1,\n",
       "         'Lynne Thomas\\nGuest': 1,\n",
       "         'Suzanne': 1,\n",
       "         'Tom Teper\\nAbsent': 1,\n",
       "         'Jeff Schrader': 1,\n",
       "         'Bill - Whatever': 1,\n",
       "         'Dennis Craig': 1,\n",
       "         'Lynne Thomas\\nJoanne': 1,\n",
       "         'Tom - There': 1,\n",
       "         'Tom – Kirstin': 1,\n",
       "         'Boxed': 1,\n",
       "         'John Laskowski\\nJoanne': 1,\n",
       "         'Lynne Thomas\\nSpace': 1,\n",
       "         'Rephrase': 1,\n",
       "         'Define': 1,\n",
       "         'Lynne - We': 1,\n",
       "         'Tom - The': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
